# -*- coding: utf-8 -*-
"""Generative.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JBYhiara46h5B0Qf6Aba_N8jdtI_8OyK
"""

# ============================================
# Install & Import Packages
# ============================================
!pip install -U pdfplumber pytesseract pillow bs4 langchain faiss-cpu sentence-transformers transformers yfinance langchain-huggingface
!pip install -U langchain langchain-community langchain-huggingface

!apt install -y tesseract-ocr

import os
import requests
import pdfplumber
import pytesseract
from PIL import Image
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.vectorstores import FAISS
from transformers import pipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
import yfinance as yf

# ============================================
# 1Ô∏è‚É£ Download Reports & Extract Tables
# ============================================
annual_report_urls = {
    "Apple": "https://app.stocklight.com/stocks/us/nasdaq-aapl/apple/annual-reports/nasdaq-aapl-2023-10K-231373899.pdf",
    "Tesla": "https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf",
    "Microsoft": "https://stocklight.com/stocks/us/nasdaq-msft/microsoft-corporation/annual-reports/nasdaq-msft-2023-10K-231118330.pdf",
    "Amazon": "https://www.annualreports.com/HostedData/AnnualReportArchive/a/NASDAQ_AMZN_2023.pdf",
    "Google": "https://abc.xyz/assets/43/44/675b83d7455885c4615d848d52a4/goog-10-k-2023.pdf"
}

def extract_text_and_tables(pdf_path):
    full_text, table_text = "", ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            # Text
            page_text = page.extract_text()
            if page_text:
                full_text += page_text + "\n"
            else:
                pil_image = page.to_image(resolution=300).original
                full_text += pytesseract.image_to_string(pil_image) + "\n"
            # Tables
            tables = page.extract_tables()
            for table in tables:
                row_strings = []
                for row in table:
                    row_strings.append(" | ".join([cell if cell else "" for cell in row]))
                joined = "\n".join(row_strings)
                table_text += joined + "\n"
                full_text += joined + "\n"
    return full_text, table_text

pdf_texts, table_texts = [], []
for company, url in annual_report_urls.items():
    print(f"üì• Downloading {company} report...")
    r = requests.get(url, stream=True)
    pdf_path = f"{company}.pdf"
    with open(pdf_path, "wb") as f:
        f.write(r.content)
    full_text, table_data = extract_text_and_tables(pdf_path)
    pdf_texts.append(full_text)
    table_texts.append(table_data)
    print(f"‚úÖ {company} report processed. Text: {len(full_text)} chars, Tables: {len(table_data)} chars")

# ============================================
# 2Ô∏è‚É£ Add Glossary & News
# ============================================
print("üìö Scraping finance glossary...")
res = requests.get("https://www.investopedia.com/financial-term-dictionary-4769738")
soup = BeautifulSoup(res.text, "html.parser")
glossary_text = "\n".join(term.text.strip() for term in soup.find_all("a", class_="dictionaryTerm"))
pdf_texts.append(glossary_text)

print("üì∞ Fetching Apple news...")
apple = yf.Ticker("AAPL")
news_text = "\n".join(f"{n.get('title')}\n{n.get('link')}" for n in apple.news[:5])
pdf_texts.append(news_text)

# ============================================
# 3Ô∏è‚É£ Chunking (Smaller to Avoid Token Errors)
# ============================================
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
full_docs, table_docs = [], []

for txt in pdf_texts:
    full_docs.extend(splitter.create_documents([txt]))

for ttxt in table_texts:
    table_docs.extend(splitter.create_documents([ttxt]))

print(f"‚úÖ Full docs: {len(full_docs)}, Table docs: {len(table_docs)}")

# ============================================
# 4Ô∏è‚É£ Vector DBs
# ============================================
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db_full = FAISS.from_documents(full_docs, embeddings)
vector_db_tables = FAISS.from_documents(table_docs, embeddings)

# ============================================
# 5Ô∏è‚É£ Prompt Template
# ============================================
prompt_template = """
You are a financial analysis assistant.
Use ONLY the provided context from company reports, glossary, and news.
If exact numbers are not found, give the closest match and say it is approximate.

Context:
{context}

Question:
{question}

Answer:
"""
PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# ============================================
# 6Ô∏è‚É£ Load FLAN-T5-Large with Safe Limits
# ============================================
print("‚è≥ Loading FLAN-T5-Large...")
pipe = pipeline("text2text-generation", model="google/flan-t5-large", max_length=300)  # Lower max_length
llm = HuggingFacePipeline(pipeline=pipe)

memory_full = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
memory_tables = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

qa_full = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vector_db_full.as_retriever(search_type="similarity", search_kwargs={"k": 4}),  # Fewer chunks
    memory=memory_full,
    combine_docs_chain_kwargs={"prompt": PROMPT}
)

qa_tables = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vector_db_tables.as_retriever(search_type="similarity", search_kwargs={"k": 4}),
    memory=memory_tables,
    combine_docs_chain_kwargs={"prompt": PROMPT}
)

# ============================================
# 7Ô∏è‚É£ Smart Router
# ============================================
financial_keywords = ["revenue", "net income", "eps", "earnings", "profit", "operating income"]

def ask(question):
    if any(word in question.lower() for word in financial_keywords):
        result = qa_tables.invoke({"question": question})
    else:
        result = qa_full.invoke({"question": question})
    print(f"\n‚ùì {question}")
    print(f"üí° {result['answer']}")

# ============================================
# 8Ô∏è‚É£ Tests
# ============================================
ask("What was Apple's total revenue in 2023?")
ask("Compare Amazon and Google's net income in 2023.")
ask("Summarize Tesla's 2023 annual report in 5 bullet points.")
ask("Explain EBITDA in simple terms.")

import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Store metrics
metrics_data = []

# ============================================
# ask() with performance metrics logging
# ============================================
def ask(question):
    start_total = time.time()

    # Route to correct retriever
    is_financial = any(word in question.lower() for word in financial_keywords)
    chain = qa_tables if is_financial else qa_full

    # Retrieval timing
    start_retrieval = time.time()
    retriever = chain.retriever
    retrieved_docs = retriever.get_relevant_documents(question)
    retrieval_time = time.time() - start_retrieval

    # LLM timing
    start_llm = time.time()
    result = chain.invoke({"question": question})
    llm_time = time.time() - start_llm

    total_time = time.time() - start_total

    # Save metrics
    metrics_data.append({
        "Question": question,
        "Retrieval Time (s)": retrieval_time,
        "LLM Time (s)": llm_time,
        "Total Time (s)": total_time,
        "Retrieved Docs": len(retrieved_docs)
    })

    # Print results
    print(f"\n‚ùì {question}")
    print(f"üí° {result['answer']}")
    print("\nüìä Performance Metrics:")
    print(f"   Retrieval time: {retrieval_time:.2f} sec")
    print(f"   LLM generation time: {llm_time:.2f} sec")
    print(f"   Total time: {total_time:.2f} sec")
    print(f"   Retrieved docs: {len(retrieved_docs)}")

# ============================================
# Test queries
# ============================================
ask("What was Apple's total revenue in 2023?")
ask("Compare Amazon and Google's net income in 2023.")
ask("Summarize Tesla's 2023 annual report in 5 bullet points.")
ask("Explain EBITDA in simple terms.")

# ============================================
# Convert to DataFrame & visualize
# ============================================
df_metrics = pd.DataFrame(metrics_data)
print("\nüìÑ Metrics DataFrame:")
print(df_metrics)

# --- Visualization 1: Grouped Bar Chart ---
sns.set(style="whitegrid")
plt.figure(figsize=(8, 5))
df_melt = df_metrics.melt(id_vars="Question", value_vars=["Retrieval Time (s)", "LLM Time (s)"],
                          var_name="Stage", value_name="Time (s)")
sns.barplot(data=df_melt, x="Question", y="Time (s)", hue="Stage")
plt.xticks(rotation=30, ha="right")
plt.title("Retrieval vs LLM Time per Question")
plt.tight_layout()
plt.show()

# --- Visualization 2: Stacked Bar Chart ---
plt.figure(figsize=(8, 5))
bar1 = plt.bar(df_metrics["Question"], df_metrics["Retrieval Time (s)"], label="Retrieval Time", color="skyblue")
bar2 = plt.bar(df_metrics["Question"], df_metrics["LLM Time (s)"],
               bottom=df_metrics["Retrieval Time (s)"], label="LLM Time", color="salmon")
plt.xticks(rotation=30, ha="right")
plt.ylabel("Time (s)")
plt.title("Total Query Latency Breakdown")
plt.legend()
plt.tight_layout()
plt.show()

import graphviz

# ============================================
# 1Ô∏è‚É£ Save metrics as CSV
# ============================================
df_metrics.to_csv("performance_metrics.csv", index=False)
print("‚úÖ Metrics saved to performance_metrics.csv")

# ============================================
# 2Ô∏è‚É£ Save charts as PNG for your report
# ============================================
# Recreate grouped bar chart
plt.figure(figsize=(8, 5))
sns.barplot(data=df_melt, x="Question", y="Time (s)", hue="Stage")
plt.xticks(rotation=30, ha="right")
plt.title("Retrieval vs LLM Time per Question")
plt.tight_layout()
plt.savefig("latency_grouped.png", dpi=300)
plt.close()

# Recreate stacked bar chart
plt.figure(figsize=(8, 5))
plt.bar(df_metrics["Question"], df_metrics["Retrieval Time (s)"], label="Retrieval Time", color="skyblue")
plt.bar(df_metrics["Question"], df_metrics["LLM Time (s)"],
        bottom=df_metrics["Retrieval Time (s)"], label="LLM Time", color="salmon")
plt.xticks(rotation=30, ha="right")
plt.ylabel("Time (s)")
plt.title("Total Query Latency Breakdown")
plt.legend()
plt.tight_layout()
plt.savefig("latency_stacked.png", dpi=300)
plt.close()

print("‚úÖ Charts saved as latency_grouped.png and latency_stacked.png")

# ============================================
# 3Ô∏è‚É£ Generate architecture diagram
# ============================================
dot = graphviz.Digraph(comment="RAG System Architecture", format="png")
dot.attr(rankdir="LR", size="8,5")

dot.node("PDFs", "Company Annual Reports", shape="folder")
dot.node("Glossary", "Finance Glossary", shape="folder")
dot.node("News", "Apple News", shape="folder")
dot.node("Chunks", "Document Chunks", shape="note")
dot.node("VectorDB", "FAISS Vector Store", shape="cylinder")
dot.node("Retriever", "Retriever", shape="diamond")
dot.node("LLM", "FLAN-T5-Large", shape="box")
dot.node("Output", "Final Answer", shape="oval")

dot.edges([
    ("PDFs", "Chunks"), ("Glossary", "Chunks"), ("News", "Chunks"),
    ("Chunks", "VectorDB"), ("VectorDB", "Retriever"),
    ("Retriever", "LLM"), ("LLM", "Output")
])

dot.render("architecture_diagram", cleanup=True)
print("‚úÖ Architecture diagram saved as architecture_diagram.png")

# ============================================
# 4Ô∏è‚É£ Save ethical considerations to text file
# ============================================
ethical_text = """
Ethical Considerations:
1. Source Reliability ‚Äì Uses official company annual reports and reputable finance sources.
2. Misinformation Avoidance ‚Äì Model outputs are restricted to retrieved context only.
3. Bias ‚Äì Data reflects the biases present in corporate reports and selected sources.
4. Privacy ‚Äì No user data is stored or shared.
5. Transparency ‚Äì Retrieved chunks can be shown to verify responses.
"""
with open("ethical_considerations.txt", "w") as f:
    f.write(ethical_text)
print("‚úÖ Ethical considerations saved as ethical_considerations.txt")